{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1d3ff3",
   "metadata": {},
   "source": [
    "# コードの概要\n",
    "\n",
    "このコードは、**「深層学習2 ― 実践的なニューラルネットワークの仕組みと実装」**という本のリポジトリ内で実装されている、**Attention機構**（注意機構）に関するPythonコードです。  \n",
    "主に、ニューラルネットワーク（とくに自然言語処理や機械翻訳など）の文脈で使われる「Attention」の計算を行うクラス群です。\n",
    "\n",
    "### 主なクラスと役割\n",
    "\n",
    "#### 1. `WeightSum`\n",
    "- **役割**: Attentionで計算した重み付き和を求めるレイヤー。\n",
    "- **forward**: 入力ベクトル群 `hs` と attention重み `a` を使って、重み付き和を計算。\n",
    "- **backward**: 勾配計算（逆伝播）を行う。\n",
    "\n",
    "#### 2. `AttentionWeight`\n",
    "- **役割**: Attentionの重み（重要度）を計算するレイヤー。\n",
    "- **forward**: 入力ベクトル群 `hs` とクエリベクトル `h` の類似度（内積）を求め、softmaxで正規化して重み `a` を計算。\n",
    "- **backward**: 勾配計算（逆伝播）。\n",
    "\n",
    "#### 3. `Attention`\n",
    "- **役割**: `AttentionWeight`と`WeightSum`を組み合わせて、Attention機構全体を実装。\n",
    "- **forward**: 重み計算→重み付き和の計算という流れ。\n",
    "- **backward**: 逆伝播の勾配計算。\n",
    "\n",
    "#### 4. `TimeAttention`\n",
    "- **役割**: 時系列データ（RNNなどで使う）に対応したAttentionレイヤー。\n",
    "- **forward**: デコーダ側の各時刻ごとにAttentionを計算し、出力に格納。\n",
    "- **backward**: 各時刻ごとに逆伝播を計算。\n",
    "\n",
    "---\n",
    "\n",
    "### まとめ\n",
    "\n",
    "- **Attentionの計算手順**  \n",
    "  1. 入力（エンコーダの隠れ状態など）とクエリ（デコーダの隠れ状態など）から重みを計算（`AttentionWeight`）。\n",
    "  2. 重みを使って入力の重み付き和を計算（`WeightSum`）。\n",
    "  3. これらを統合して、Attention層（`Attention`）として利用。\n",
    "  4. 時系列データに対応するため、各時刻ごとにAttention計算（`TimeAttention`）。\n",
    "\n",
    "- **用途**  \n",
    "  主にSeq2Seqモデルなどで、入力系列のどの部分に着目すべきかを学習するために利用されます。\n",
    "\n",
    "---\n",
    "\n",
    "もし、「どの部分が知りたい」「具体的な処理内容」「使い方」など質問があれば、さらに詳しく解説できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.np import *\n",
    "from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca307e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self) -> None:\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self) -> None:\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a526575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4583136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
