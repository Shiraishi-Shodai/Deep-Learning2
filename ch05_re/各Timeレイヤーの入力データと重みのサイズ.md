# SimpleRnnlmのTimeレイヤー詳細と処理フロー

## 1. 各Timeレイヤーのデータサイズと重みサイズ

記号の定義:
- **N**: バッチサイズ (Batch Size)
- **T**: 時間サイズ (Time Size)
- **V**: 単語数 (Vocab Size)
- **D**: 単語ベクトルの次元数 (Word Vector Size)
- **H**: 隠れ状態の次元数 (Hidden Size)

### TimeEmbedding レイヤ
- **入力 (`xs`)**: `(N, T)` - 単語IDのリスト
- **重み (`embed_W`)**: `(V, D)` - 単語埋め込み行列
- **出力**: `(N, T, D)` - 単語ベクトル

### TimeRNN レイヤ
- **入力**: `(N, T, D)` - TimeEmbeddingからの出力
- **重み**:
    - `rnn_Wx`: `(D, H)` - 入力に対する重み
    - `rnn_Wh`: `(H, H)` - 前の時刻の隠れ状態に対する重み
    - `rnn_b`: `(H,)` - バイアス
- **出力 (`hs`)**: `(N, T, H)` - 各時刻の隠れ状態

### TimeAffine レイヤ
- **入力**: `(N, T, H)` - TimeRNNからの出力
- **重み**:
    - `affine_W`: `(H, V)` - アフィン変換の重み
    - `affine_b`: `(V,)` - アフィン変換のバイアス
- **出力**: `(N, T, V)` - スコア（各単語の出現確率の元となる値）

### TimeSoftmaxWithLoss レイヤ
- **入力**: 
    - `xs`: `(N, T, V)` - TimeAffineからの出力（スコア）
    - `ts`: `(N, T)` - 正解ラベル（単語ID）
- **出力**: `Loss` (スカラ) - 損失値

---

## 2. 処理の流れとイメージ（順伝搬・逆伝搬）

各レイヤーがどのような役割を持ち、どうデータが流れるのか、イメージで解説します。

### 🟢 1. TimeEmbedding レイヤ
**役割: 「単語IDを、意味を持つベクトルに変換する」**

#### ➡️ 順伝搬 (Forward)
*   **処理**: 単語ID（数字）に対応する行を、重み行列 `W` から抜き出します。
*   **流れ**: `xs (単語ID)` ➔ **[抽出]** ➔ `out (単語ベクトル)`

#### ⬅️ 逆伝搬 (Backward)
*   **処理**: 逆伝搬してきた勾配を、使われた単語IDの場所に足し合わせます（`np.add.at`のような処理）。
*   **流れ**: `dout` ➔ **[集計]** ➔ `dW (重みの勾配)`

> **🧒 小学生でもわかる例え話**
> *   **順伝搬**: クラスの出席番号（ID）を聞いて、ロッカーからその人の「プロフィールカード（ベクトル）」を取り出す作業です。「1番！」と言われたら1番のカードを取り出します。
> *   **逆伝搬**: 先生から「このカードの情報、ちょっと修正して」とメモ（勾配）を渡されたら、元のロッカーの場所にそのメモを挟んでおく作業です。

---

### 🔄 2. TimeRNN レイヤ
**役割: 「過去の記憶と現在の情報を混ぜて、新しい記憶を作る」**

#### ➡️ 順伝搬 (Forward)
*   **処理**: 「今の入力 `x`」と「1つ前の記憶 `h_prev`」を混ぜ合わせ、活性化関数 `tanh` を通して「新しい記憶 `h_next`」を作ります。これを時間 `T` 回繰り返します。
*   **流れ**: `xs` + `h_prev` ➔ **[合体・変換]** ➔ `hs (全時刻の記憶)`

#### ⬅️ 逆伝搬 (Backward)
*   **処理**: 「未来からの勾配」と「上の層からの勾配」を合わせて、過去へ伝えます（BPTT）。
*   **流れ**: `dhs` + `dh_next` ➔ **[分解・過去へ]** ➔ `dxs`, `dh_prev`

> **🧒 小学生でもわかる例え話**
> *   **順伝搬（伝言ゲーム）**: 前の人から聞いた話（過去の記憶）に、自分が新しく見たヒント（入力）を付け加えて、次の人に伝えるゲームです。これを繰り返すと、最後の人はすべての情報を持った状態になります。
> *   **逆伝搬（反省会）**: 最後の人から「話が違うよ！」と怒られたら（勾配）、「ごめん、僕の聞き間違いかも」と前の人に伝え、さらにその前の人へ…と原因を遡っていく作業です。

---

### 📐 3. TimeAffine レイヤ
**役割: 「記憶（抽象的なイメージ）を、具体的な単語の候補（スコア）に変換する」**

#### ➡️ 順伝搬 (Forward)
*   **処理**: RNNが作った記憶 `h` に行列演算を行い、単語数分のスコア（確率の元）を計算します。
*   **流れ**: `hs` ➔ **[行列積]** ➔ `xs (スコア)`

#### ⬅️ 逆伝搬 (Backward)
*   **処理**: スコアの修正指示を受け取り、それを記憶の修正指示に変換します。
*   **流れ**: `dout` ➔ **[行列積]** ➔ `dhs`

> **🧒 小学生でもわかる例え話**
> *   **順伝搬**: RNN君が持っている「ふわっとしたイメージ（記憶）」を、具体的な「言葉のリスト」に書き換える作業です。「なんとなく甘くて赤い果物…」というイメージから、「りんご: 90点, いちご: 80点, みかん: 10点」のような点数表を作ります。
> *   **逆伝搬**: 「りんごの点数をもっと上げて！」と言われたら、元の「甘くて赤い果物」というイメージをどう修正すればいいかを計算します。

---

### 🎯 4. TimeSoftmaxWithLoss レイヤ
**役割: 「スコアを確率に直し、正解と比較して間違い（損失）を計算する」**

#### ➡️ 順伝搬 (Forward)
*   **処理**: スコアを `Softmax` で確率（％）に変換し、正解ラベル `ts` と比較して、どれくらい間違っているか（交差エントロピー誤差）を計算します。
*   **流れ**: `xs (スコア)` + `ts (正解)` ➔ **[確率変換・比較]** ➔ `Loss (損失)`

#### ⬅️ 逆伝搬 (Backward)
*   **処理**: 「予測確率」と「正解（100%）」のズレ `y - t` を計算し、それを勾配として返します。ここが逆伝搬のスタート地点です。
*   **流れ**: `1 (スタート)` ➔ **[ズレを計算]** ➔ `dout (勾配)`

> **🧒 小学生でもわかる例え話**
> *   **順伝搬（テストの採点）**: Affine君が作った点数表（スコア）を見て、「りんごである確率 90%」と予想しました。でも正解が「いちご」だったら、「うわ、間違えた！悔しい！（損失）」となります。
> *   **逆伝搬（復習）**: 「いちごが正解なのに、りんごって答えちゃった」という「ズレ」を、前のAffine君に「もっといちごの点数を高くしてよ！」と文句（勾配）として伝えます。
